{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdYPhRDcg0jp/NuqiATQfb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/hopfield_boltzmann/blob/main/section_3/03_rbm_class.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 制限ボルツマンマシンをPythonのクラスで実装する\n",
        "可視層 (Visible Layer) と 隠れ層 (Hidden Layer) の2層構造を持つ制限ボルツマンマシン (RBM) を、Pythonのクラスで実装します。  \n",
        "こRBM のクラス `RestrictedBoltzmannMachine` がどのように動作し、各メソッドが何をしているのかを解説します。"
      ],
      "metadata": {
        "id": "D1ewu_ttnVop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BRestrictedBoltzmannMachineクラス\n",
        "- **可視層 (Visible Layer)** と **隠れ層 (Hidden Layer)** の2層構造を持つ制限ボルツマンマシン (RBM) を実装しています。  \n",
        "- **Contrastive Divergence (CD)** アルゴリズムを用いて、データから重みとバイアスを学習します。  \n",
        "- 温度パラメータを導入して、学習時の探索の幅を調整します。  "
      ],
      "metadata": {
        "id": "hs-EOPaKnf9A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2vttcSpLchW"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "class RestrictedBoltzmannMachine:\n",
        "    def __init__(self, n_visible, n_hidden, learning_rate=0.1, temperature=1.0, epochs=1000):\n",
        "        \"\"\"RBMの初期化\"\"\"\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.learning_rate = learning_rate\n",
        "        self.temperature = temperature  # 温度パラメータ\n",
        "        self.epochs = epochs\n",
        "\n",
        "        # 重み行列の初期化（標準正規分布で初期化）\n",
        "        self.weights = np.random.randn(n_hidden, n_visible) * 0.1\n",
        "        # バイアスの初期化\n",
        "        self.visible_bias = np.zeros(n_visible)\n",
        "        self.hidden_bias = np.zeros(n_hidden)\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"温度パラメータを考慮したシグモイド関数\"\"\"\n",
        "        return 1 / (1 + np.exp(-x / self.temperature))\n",
        "\n",
        "    def sample_hidden(self, visible):\n",
        "        \"\"\"可視層から隠れ層へのサンプリング\"\"\"\n",
        "        activation = np.dot(visible, self.weights.T) + self.hidden_bias\n",
        "        prob_hidden = self.sigmoid(activation)\n",
        "        hidden_state = (np.random.rand(self.n_hidden) < prob_hidden).astype(np.int_)\n",
        "        return prob_hidden, hidden_state\n",
        "\n",
        "    def sample_visible(self, hidden):\n",
        "        \"\"\"隠れ層から可視層へのサンプリング\"\"\"\n",
        "        activation = np.dot(hidden, self.weights) + self.visible_bias\n",
        "        prob_visible = self.sigmoid(activation)\n",
        "        visible_state = (np.random.rand(self.n_visible) < prob_visible).astype(np.int_)\n",
        "        return prob_visible, visible_state\n",
        "\n",
        "    def contrastive_divergence(self, input_data):\n",
        "        \"\"\"Contrastive Divergence法でRBMを学習\"\"\"\n",
        "        prob_hidden, hidden_state = self.sample_hidden(input_data)\n",
        "        prob_visible, visible_state = self.sample_visible(hidden_state)\n",
        "        prob_hidden_reconstructed, _ = self.sample_hidden(visible_state)\n",
        "\n",
        "        # 勾配計算と重み更新\n",
        "        positive_grad = np.outer(prob_hidden, input_data)\n",
        "        negative_grad = np.outer(prob_hidden_reconstructed, visible_state)\n",
        "\n",
        "        self.weights += self.learning_rate * (positive_grad - negative_grad)\n",
        "        self.visible_bias += self.learning_rate * (input_data - visible_state)\n",
        "        self.hidden_bias += self.learning_rate * (prob_hidden - prob_hidden_reconstructed)\n",
        "\n",
        "    def train(self, data):\n",
        "        \"\"\"RBMのトレーニング\"\"\"\n",
        "        for epoch in range(self.epochs):\n",
        "            for sample in data:\n",
        "                self.contrastive_divergence(sample)\n",
        "\n",
        "            # 温度を徐々に下げる（アニーリング）\n",
        "            self.temperature *= 0.99\n",
        "\n",
        "            if (epoch + 1) % 100 == 0:\n",
        "                print(f\"Epoch {epoch + 1}/{self.epochs}, Temperature: {self.temperature:.4f}\")\n",
        "\n",
        "    def run_hidden(self, data):\n",
        "        \"\"\"可視層から隠れ層の表現を取得\"\"\"\n",
        "        _, hidden_state = self.sample_hidden(data)\n",
        "        return hidden_state\n",
        "\n",
        "    def run_visible(self, hidden):\n",
        "        \"\"\"隠れ層から可視層を再構成\"\"\"\n",
        "        _, visible_state = self.sample_visible(hidden)\n",
        "        return visible_state"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下、各メソッドを解説します。\n",
        "\n",
        "### **1. `__init__` メソッド**\n",
        "```python\n",
        "def __init__(self, n_visible, n_hidden, learning_rate=0.1, temperature=1.0, epochs=1000):\n",
        "    \"\"\"RBMの初期化\"\"\"\n",
        "    self.n_visible = n_visible\n",
        "    self.n_hidden = n_hidden\n",
        "    self.learning_rate = learning_rate\n",
        "    self.temperature = temperature  # 温度パラメータ\n",
        "    self.epochs = epochs\n",
        "\n",
        "    # 重み行列の初期化（正規分布に従うランダムな小さい値で初期化）\n",
        "    self.weights = np.random.randn(n_hidden, n_visible) * 0.1\n",
        "    # 可視層と隠れ層のバイアス初期化（0に初期化）\n",
        "    self.visible_bias = np.zeros(n_visible)\n",
        "    self.hidden_bias = np.zeros(n_hidden)\n",
        "```\n",
        "\n",
        "#### **解説**\n",
        "- **`n_visible`**: 可視層のノード数（例えば、MNIST の場合は 28×28 = 784）。\n",
        "- **`n_hidden`**: 隠れ層のノード数（抽出する特徴の数）。\n",
        "- **`learning_rate`**: 重みやバイアスの更新の際の学習率。\n",
        "- **`temperature`**: 温度パラメータ。値が大きいと確率が平坦になり、探索的な学習になります。値が小さいと確率が鋭くなり、確定的な学習になります。\n",
        "- **`epochs`**: 学習のエポック数。\n",
        "- **`weights`**: 可視層と隠れ層の間の重み行列（`n_hidden × n_visible`）。\n",
        "- **`visible_bias`**: 可視層のバイアス。\n",
        "- **`hidden_bias`**: 隠れ層のバイアス。\n",
        "\n",
        "---\n",
        "\n",
        "### **2. `sigmoid` メソッド**\n",
        "```python\n",
        "def sigmoid(self, x):\n",
        "    \"\"\"温度パラメータを考慮したシグモイド関数\"\"\"\n",
        "    return 1 / (1 + np.exp(-x / self.temperature))\n",
        "```\n",
        "\n",
        "#### **解説**\n",
        "- **シグモイド関数**は、入力値 `x` を 0 から 1 の範囲の確率に変換します。\n",
        "- 温度パラメータ `self.temperature` を使用して、関数の傾きを調整します。\n",
        "\n",
        "---\n",
        "\n",
        "### **3. `sample_hidden` メソッド**\n",
        "```python\n",
        "def sample_hidden(self, visible):\n",
        "    \"\"\"可視層から隠れ層へのサンプリング\"\"\"\n",
        "    activation = np.dot(visible, self.weights.T) + self.hidden_bias\n",
        "    prob_hidden = self.sigmoid(activation)\n",
        "    hidden_state = (np.random.rand(self.n_hidden) < prob_hidden).astype(np.int_)\n",
        "    return prob_hidden, hidden_state\n",
        "```\n",
        "\n",
        "#### **解説**\n",
        "- **目的**: 可視層の入力から隠れ層のノードをサンプリングする。\n",
        "- **手順**:\n",
        "  1. **活性化関数の計算**:\n",
        "     - `activation = np.dot(visible, self.weights.T) + self.hidden_bias`\n",
        "     - 可視層の入力と重み行列の内積を計算し、隠れ層のバイアスを加えます。\n",
        "  2. **シグモイド関数を通して確率に変換**。\n",
        "  3. **確率に基づいて0または1にサンプリング**します（`np.random.rand()` を用いた確率的サンプリング）。\n",
        "\n",
        "---\n",
        "\n",
        "### **4. `sample_visible` メソッド**\n",
        "```python\n",
        "def sample_visible(self, hidden):\n",
        "    \"\"\"隠れ層から可視層へのサンプリング\"\"\"\n",
        "    activation = np.dot(hidden, self.weights) + self.visible_bias\n",
        "    prob_visible = self.sigmoid(activation)\n",
        "    visible_state = (np.random.rand(self.n_visible) < prob_visible).astype(np.int_)\n",
        "    return prob_visible, visible_state\n",
        "```\n",
        "\n",
        "#### **解説**\n",
        "- **目的**: 隠れ層の状態から可視層のノードをサンプリングする。\n",
        "- **手順**は `sample_hidden` メソッドと同様です。\n",
        "\n",
        "---\n",
        "\n",
        "### **5. `contrastive_divergence` メソッド**\n",
        "```python\n",
        "def contrastive_divergence(self, input_data):\n",
        "    \"\"\"Contrastive Divergence法でRBMを学習\"\"\"\n",
        "    prob_hidden, hidden_state = self.sample_hidden(input_data)\n",
        "    prob_visible, visible_state = self.sample_visible(hidden_state)\n",
        "    prob_hidden_reconstructed, _ = self.sample_hidden(visible_state)\n",
        "\n",
        "    # 勾配計算と重み更新\n",
        "    positive_grad = np.outer(prob_hidden, input_data)\n",
        "    negative_grad = np.outer(prob_hidden_reconstructed, visible_state)\n",
        "\n",
        "    self.weights += self.learning_rate * (positive_grad - negative_grad)\n",
        "    self.visible_bias += self.learning_rate * (input_data - visible_state)\n",
        "    self.hidden_bias += self.learning_rate * (prob_hidden - prob_hidden_reconstructed)\n",
        "```\n",
        "\n",
        "#### **解説**\n",
        "- **Contrastive Divergence (CD)** を使用して RBM のパラメータを更新します。\n",
        "- **手順**:\n",
        "  1. **正方向パス**: 入力から隠れ層の状態を計算。\n",
        "  2. **負方向パス**: 隠れ層から可視層を再構成し、再度隠れ層の状態を計算。\n",
        "  3. **重みの更新**:\n",
        "     - 正方向パスと負方向パスの勾配の差を用いて重みとバイアスを更新。\n",
        "\n",
        "---\n",
        "\n",
        "### **6. `train` メソッド**\n",
        "```python\n",
        "def train(self, data):\n",
        "    \"\"\"RBMのトレーニング\"\"\"\n",
        "    for epoch in range(self.epochs):\n",
        "        for sample in data:\n",
        "            self.contrastive_divergence(sample)\n",
        "        \n",
        "        # 温度を徐々に下げる（アニーリング）\n",
        "        self.temperature *= 0.99\n",
        "        \n",
        "        if (epoch + 1) % 100 == 0:\n",
        "            print(f\"Epoch {epoch + 1}/{self.epochs}, Temperature: {self.temperature:.4f}\")\n",
        "```\n",
        "\n",
        "#### **解説**\n",
        "- **データセット全体に対して Contrastive Divergence を繰り返し適用**し、RBM の重みとバイアスを学習します。\n",
        "- 温度パラメータを **エポックごとに少しずつ減少**（アニーリング）させることで、モデルの収束を促します。\n",
        "\n",
        "---\n",
        "\n",
        "### **7. `run_hidden` および `run_visible` メソッド**\n",
        "```python\n",
        "def run_hidden(self, data):\n",
        "    \"\"\"可視層から隠れ層の表現を取得\"\"\"\n",
        "    _, hidden_state = self.sample_hidden(data)\n",
        "    return hidden_state\n",
        "\n",
        "def run_visible(self, hidden):\n",
        "    \"\"\"隠れ層から可視層を再構成\"\"\"\n",
        "    _, visible_state = self.sample_visible(hidden)\n",
        "    return visible_state\n",
        "```\n",
        "\n",
        "#### **解説**\n",
        "- **`run_hidden`**: 学習した RBM から、可視層の入力に対して隠れ層の特徴ベクトルを取得。\n",
        "- **`run_visible`**: 隠れ層の特徴ベクトルから、元の可視層の再構成を行う。"
      ],
      "metadata": {
        "id": "lupKR1TavQJK"
      }
    }
  ]
}